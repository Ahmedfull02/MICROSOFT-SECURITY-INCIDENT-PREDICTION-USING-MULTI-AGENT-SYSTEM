{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 8929038,
          "sourceType": "datasetVersion",
          "datasetId": 5371198
        }
      ],
      "dockerImageVersionId": 30918,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Practica-SIGE",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahmedfull02/MICROSOFT-SECURITY-INCIDENT-PREDICTION-USING-MULTI-AGENT-SYSTEM/blob/master/Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "microsoft_microsoft_security_incident_prediction_path = kagglehub.dataset_download('Microsoft/microsoft-security-incident-prediction')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "Ggm4JGTDPD2f"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MICROSOFT SECURITY INCIDENT PREDICTION\n",
        "\n",
        "BY: MARINA J. CARRANZA SÁNCHEZ & CARLOTA DE LA VEGA SORIANO"
      ],
      "metadata": {
        "id": "Q3fxx3MAPD2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Preparación del entorno"
      ],
      "metadata": {
        "id": "8D6WxOM7PD2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1. Importaciones"
      ],
      "metadata": {
        "id": "qaZCvTNlPD2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-30T12:47:03.552909Z",
          "iopub.execute_input": "2025-05-30T12:47:03.553189Z",
          "iopub.status.idle": "2025-05-30T12:47:04.56493Z",
          "shell.execute_reply.started": "2025-05-30T12:47:03.553157Z",
          "shell.execute_reply": "2025-05-30T12:47:04.564225Z"
        },
        "id": "AlYTUQS8PD2r"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2. Estilo de las gráficas"
      ],
      "metadata": {
        "id": "EAgQ4JdzPD2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def style_and_save(\n",
        "    ax,\n",
        "    title='',\n",
        "    xlabel='',\n",
        "    ylabel='',\n",
        "    filename='plot.png',\n",
        "    title_pad=20,\n",
        "    xlabel_pad=15,\n",
        "    ylabel_pad=15,\n",
        "    xtick_rotation=0,\n",
        "    ytick_rotation=0,\n",
        "    grid_axis='y',\n",
        "    fontsize=11,\n",
        "    show_legend=True\n",
        "):\n",
        "    ax.set_title(title, fontsize=12, fontweight='bold', pad=title_pad)\n",
        "    ax.set_xlabel(xlabel, fontsize=10, fontweight='semibold', labelpad=xlabel_pad)\n",
        "    ax.set_ylabel(ylabel, fontsize=10, fontweight='semibold', labelpad=ylabel_pad)\n",
        "    ax.tick_params(axis='x', labelrotation=xtick_rotation, labelsize=fontsize)\n",
        "    ax.tick_params(axis='y', labelrotation=ytick_rotation, labelsize=fontsize)\n",
        "    ax.grid(axis=grid_axis, linestyle='--', alpha=0.5)\n",
        "    if show_legend:\n",
        "        ax.legend(loc='lower right', fontsize=10)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename, dpi=300)\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-30T12:47:04.566147Z",
          "iopub.execute_input": "2025-05-30T12:47:04.56659Z",
          "iopub.status.idle": "2025-05-30T12:47:04.572064Z",
          "shell.execute_reply.started": "2025-05-30T12:47:04.566535Z",
          "shell.execute_reply": "2025-05-30T12:47:04.571363Z"
        },
        "id": "NfBSH3FpPD2u"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def create_count_plot(\n",
        "    df,\n",
        "    column,\n",
        "    title='',\n",
        "    xlabel='',\n",
        "    ylabel='',\n",
        "    filename='count_plot.png',\n",
        "    order=None,\n",
        "    xtick_rotation=0,\n",
        "    annotate_values=False,\n",
        "    figsize=(8, 5)\n",
        "):\n",
        "    plt.figure(figsize=figsize)\n",
        "    ax = sns.countplot(\n",
        "        data=df[df[column].notnull()],\n",
        "        x=column,\n",
        "        order=order,\n",
        "        edgecolor='gray',\n",
        "        palette='pastel'\n",
        "    )\n",
        "\n",
        "    if annotate_values:\n",
        "        for bar in ax.patches:\n",
        "            height = bar.get_height()\n",
        "            ax.annotate(f'{height:,}',\n",
        "                        (bar.get_x() + bar.get_width() / 2., height),\n",
        "                        ha='center', va='bottom', fontsize=10, fontweight='semibold')\n",
        "\n",
        "    style_and_save(\n",
        "        ax,\n",
        "        title=title,\n",
        "        xlabel=xlabel,\n",
        "        ylabel=ylabel,\n",
        "        filename=filename,\n",
        "        xtick_rotation=xtick_rotation\n",
        "    )\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-30T12:47:04.573659Z",
          "iopub.execute_input": "2025-05-30T12:47:04.573876Z",
          "iopub.status.idle": "2025-05-30T12:47:04.588345Z",
          "shell.execute_reply.started": "2025-05-30T12:47:04.57385Z",
          "shell.execute_reply": "2025-05-30T12:47:04.587673Z"
        },
        "id": "bTBlVcNCPD2w"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def create_stacked_barh(\n",
        "    df,\n",
        "    title='',\n",
        "    xlabel='',\n",
        "    ylabel='',\n",
        "    filename='stacked_barh.png',\n",
        "    colors=None,\n",
        "    figsize=(10, 6),\n",
        "    annotate_values=False\n",
        "):\n",
        "    if colors is None:\n",
        "        colors = sns.color_palette('pastel')[:df.shape[1]]\n",
        "\n",
        "    ax = df.plot(\n",
        "        kind='barh',\n",
        "        stacked=True,\n",
        "        figsize=figsize,\n",
        "        color=colors,\n",
        "        edgecolor='gray'\n",
        "    )\n",
        "\n",
        "    if annotate_values:\n",
        "        for container in ax.containers:\n",
        "            for bar in container:\n",
        "                width = bar.get_width()\n",
        "                if width > 0:\n",
        "                    ax.text(\n",
        "                        bar.get_x() + width / 2,\n",
        "                        bar.get_y() + bar.get_height() / 2,\n",
        "                        f'{int(width):,}',\n",
        "                        ha='center', va='center',\n",
        "                        fontsize=9\n",
        "                    )\n",
        "\n",
        "    style_and_save(\n",
        "        ax,\n",
        "        title=title,\n",
        "        xlabel=xlabel,\n",
        "        ylabel=ylabel,\n",
        "        filename=filename,\n",
        "        grid_axis='x'\n",
        "    )"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-30T12:47:04.589414Z",
          "iopub.execute_input": "2025-05-30T12:47:04.589688Z",
          "iopub.status.idle": "2025-05-30T12:47:04.603806Z",
          "shell.execute_reply.started": "2025-05-30T12:47:04.589661Z",
          "shell.execute_reply": "2025-05-30T12:47:04.603159Z"
        },
        "id": "PuedpHyiPD2x"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3. Carga del dataset"
      ],
      "metadata": {
        "id": "GTgk9riWPD2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/kaggle/input/microsoft-security-incident-prediction/GUIDE_Train.csv')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-30T12:47:04.604694Z",
          "iopub.execute_input": "2025-05-30T12:47:04.604967Z"
        },
        "id": "2xskHvbSPD20"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Analisis exploratorio"
      ],
      "metadata": {
        "id": "2jjOXbo_PD28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. Definición del dataset"
      ],
      "metadata": {
        "id": "2cwfxnNCPD29"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.1. Variables (columnas)"
      ],
      "metadata": {
        "id": "fICbAT2DPD2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "trusted": true,
        "id": "BCWS71vEPD2-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the description for the features, as mentioned in the source paper titled [**AI-Driven Guided Response for Security Operation Centers with Microsoft Copilot for Security**](https://arxiv.org/pdf/2407.09017).\n",
        "| **Feature**            | **Description**                                                                                               |\n",
        "| ---------------------- | ------------------------------------------------------------------------------------------------------------- |\n",
        "| **Id**                 | Unique identifier for each OrgId-IncidentId combination, ensuring global uniqueness of each record.           |\n",
        "| **OrgId**              | Identifier for the organization where the incident or alert originated, used to separate data across tenants. |\n",
        "| **IncidentId**         | Unique identifier assigned by the organization to each tracked or investigated security incident.             |\n",
        "| **AlertId**            | Unique identifier for an individual alert generated by a detection system or security tool.                   |\n",
        "| **Timestamp**          | Date and time when the alert was created or logged in the system, in UTC format.                              |\n",
        "| **DetectorId**         | Identifier of the detection engine or system that generated the alert.                                        |\n",
        "| **AlertTitle**         | Short descriptive title summarizing the nature of the alert.                                                  |\n",
        "| **Category**           | Broad classification or category of the alert, indicating the type of detected threat or issue.               |\n",
        "| **MitreTechniques**    | List of MITRE ATT\\&CK techniques associated with the alert, describing the tactics involved.                  |\n",
        "| **IncidentGrade**      | Severity level assigned to the incident by the Security Operations Center after review.                       |\n",
        "| **ActionGrouped**      | High-level description of the remediation or response action taken.                                           |\n",
        "| **ActionGranular**     | Detailed description of the remediation action taken, specifying exact measures.                              |\n",
        "| **EntityType**         | Type of entity involved in the alert.                                                                         |\n",
        "| **EvidenceRole**       | Role of the evidence within the investigation.                                                                |\n",
        "| **Roles**              | Additional metadata or labels indicating other roles played by the entity in the alert or investigation.      |\n",
        "| **DeviceId**           | Unique identifier assigned to the device involved in the alert.                                               |\n",
        "| **DeviceName**         | Human-readable name of the device involved in the alert.                                                      |\n",
        "| **Sha256**             | SHA-256 hash of the file involved in the alert.                                                               |\n",
        "| **IpAddress**          | IP address associated with the alert.                                                                         |\n",
        "| **Url**                | URL involved in the alert.                                                                                    |\n",
        "| **AccountSid**         | Identifier for an on-premises account linked to the alert or incident.                                        |\n",
        "| **AccountUpn**         | User Principal Name of the account involved in the alert.                                                     |\n",
        "| **AccountObjectId**    | Entra ID object identifier for the user or account involved.                                                  |\n",
        "| **AccountName**        | Username of the on-premises account involved in the alert.                                                    |\n",
        "| **NetworkMessageId**   | Organization-level unique identifier for an email message involved in the alert.                              |\n",
        "| **EmailClusterId**     | Identifier for a cluster or group of related email messages involved in the incident.                         |\n",
        "| **RegistryKey**        | Windows registry key involved in the alert.                                                                   |\n",
        "| **RegistryValueName**  | Name of the registry value that was modified, created, or accessed.                                           |\n",
        "| **RegistryValueData**  | Data stored in the registry value at the time of the alert.                                                   |\n",
        "| **ApplicationId**      | Unique identifier for an application involved in the alert.                                                   |\n",
        "| **ApplicationName**    | Name of the application involved in the alert or incident.                                                    |\n",
        "| **OAuthApplicationId** | Identifier for the OAuth application involved.                                                                |\n",
        "| **ThreatFamily**       | Name of the malware family detected as part of the alert.                                                     |\n",
        "| **FileName**           | Name of the file involved in the alert.                                                                       |\n",
        "| **FolderPath**         | Path where the file was located at the time of the alert.                                                     |\n",
        "| **ResourceIdName**     | Name of the Azure resource involved in the alert or investigation.                                            |\n",
        "| **ResourceType**       | Type of Azure resource involved.                                                                              |\n",
        "| **OSFamily**           | Family of the operating system running on the device involved.                                                |\n",
        "| **OSVersion**          | Version of the operating system running on the device involved.                                               |\n",
        "| **AntispamDirection**  | Direction of the email traffic as processed by the antispam system.                                           |\n",
        "| **SuspicionLevel**     | Assigned level of suspicion for the alert or entity.                                                          |\n",
        "| **LastVerdict**        | Final determination or classification for the alert after review.                                             |\n",
        "| **CountryCode**        | ISO country code indicating the geographic location associated with the alert.                                |\n",
        "| **State**              | State or province name associated with the geographic location of the alert evidence.                         |\n",
        "| **City**               | City name associated with the geographic location of the alert evidence.                                      |\n"
      ],
      "metadata": {
        "id": "zX40FIUEPD2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.2. Filas"
      ],
      "metadata": {
        "id": "7GSZfc0-PD2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_rows = len(df)\n",
        "print(total_rows)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Xyv5lB9sPD2_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. Elección de la variable objetivo"
      ],
      "metadata": {
        "id": "bCBBy-oIPD2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "variables_candidatas = ['IncidentGrade', 'SuspicionLevel', 'LastVerdict', 'Category']\n",
        "\n",
        "for col in variables_candidatas:\n",
        "    print(f\"\\n📌 Variable: {col}\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"Valores únicos:\\n{df[col].unique()}\")\n",
        "    print(f\"\\nNúmero de valores nulos: {df[col].isnull().sum()} de {len(df)} registros totales\")\n",
        "    print(\"=\" * 60)"
      ],
      "metadata": {
        "trusted": true,
        "id": "dOGfo7LgPD3A"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "variables = ['IncidentGrade', 'SuspicionLevel', 'LastVerdict', 'Category']\n",
        "\n",
        "# Cálculo de nulos y no nulos\n",
        "\n",
        "data = {\n",
        "    'Not Null': [df[var].notnull().sum() for var in variables],\n",
        "    'Null': [df[var].isnull().sum() for var in variables]\n",
        "}\n",
        "\n",
        "# Creación del dataframe\n",
        "\n",
        "df_nulls = pd.DataFrame(data, index=variables)\n",
        "\n",
        "# Gráfico\n",
        "\n",
        "create_stacked_barh(\n",
        "    df=df_nulls,\n",
        "    title='Valores Nulos vs No Nulos por variable candidata',\n",
        "    xlabel='Número de registros',\n",
        "    ylabel='Variable',\n",
        "    filename='valores_nulos_por_variable.png',\n",
        "    annotate_values=False\n",
        ")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "p6_ILug3PD3A"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3. Distribución de la variable objetivo"
      ],
      "metadata": {
        "id": "jLqIU68jPD3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "create_count_plot(\n",
        "    df=df,\n",
        "    column='IncidentGrade',\n",
        "    title='Distribución de la variable objetivo: IncidentGrade',\n",
        "    xlabel='Clase',\n",
        "    ylabel='Cantidad de registros',\n",
        "    filename='distribucion_variable_objetivo.png',\n",
        "    order=df['IncidentGrade'].value_counts().index\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "617XqncwPD3B"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4. Visualización de valores nulos"
      ],
      "metadata": {
        "id": "tOlu0iooPD3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Contar los valores nulos por columna\n",
        "\n",
        "missing_values = df.isnull().sum()\n",
        "missing_values = missing_values[missing_values > 0]\n",
        "missing_values = missing_values.sort_values()\n",
        "\n",
        "# Visualizar\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "missing_values.plot(kind='barh', color='skyblue')\n",
        "plt.title('Número de valores faltantes por columna')\n",
        "plt.xlabel('Cantidad de nulos')\n",
        "plt.ylabel('Columnas')\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "IGWN3bUkPD3C"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Preprocesamiento"
      ],
      "metadata": {
        "id": "y5iLz-AQPD3D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. Limpieza de datos"
      ],
      "metadata": {
        "id": "2cwQwmxKPD3D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.1. Eliminar filas/columnas por valores nulos"
      ],
      "metadata": {
        "id": "2sYmeQCWPD3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def viewRemainingColumns():\n",
        "    numeric_columns = df.select_dtypes(include=['float64', 'int64', 'int32']).columns\n",
        "    string_columns = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "    # Imprimir los nombres de las columnas\n",
        "    print(\"Columnas numéricas:\")\n",
        "    print(numeric_columns)\n",
        "    print(f\"Total de columnas numéricas: {len(numeric_columns)}\\n\")\n",
        "\n",
        "    print(\"Columnas de tipo string:\")\n",
        "    print(string_columns)\n",
        "    print(f\"Total de columnas de tipo string: {len(string_columns)}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "r_OFnsTsPD3D"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminar columnas con más del 70% de valores nulos\n",
        "\n",
        "columns_with_high_null = []\n",
        "\n",
        "for column in df.columns:\n",
        "    if df[column].isnull().sum() / total_rows > 0.7:\n",
        "        columns_with_high_null.append(column)\n",
        "\n",
        "print(\"Columnas con más del 70% de valores nulos:\", columns_with_high_null)\n",
        "print(f\"Total de columnas a eliminar: {len(columns_with_high_null)}\\n\")\n",
        "\n",
        "df = df.drop(columns=columns_with_high_null)"
      ],
      "metadata": {
        "trusted": true,
        "id": "s_cbSGoqPD3E"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "viewRemainingColumns()"
      ],
      "metadata": {
        "trusted": true,
        "id": "6zoNQ9srPD3E"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Borrar filas cuyo Incident Grade sea nulo para no inventarnos la clase objetivo\n",
        "\n",
        "df = df.dropna(subset=['IncidentGrade'])\n",
        "\n",
        "total_rows_after_delete = len(df)\n",
        "print(f\"{round((total_rows_after_delete / total_rows * 100), 2)}%\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "d_8s5oPOPD3F"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Actualizar número total de filas\n",
        "\n",
        "total_rows = total_rows_after_delete"
      ],
      "metadata": {
        "trusted": true,
        "id": "svyW9ZeePD3F"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.2. Imputación de valores nulos\n"
      ],
      "metadata": {
        "id": "diKUW809PD3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imputar los valores (con la moda) de MitreTechniques\n",
        "\n",
        "print(df['MitreTechniques'].isnull().sum())\n",
        "\n",
        "mode = df['MitreTechniques'].mode()[0]\n",
        "\n",
        "df.fillna({'MitreTechniques': mode}, inplace=True)\n",
        "\n",
        "print(df['MitreTechniques'].isnull().sum())"
      ],
      "metadata": {
        "trusted": true,
        "id": "c_0f2NZwPD3G"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.3. Eliminar columnas irrelevantes"
      ],
      "metadata": {
        "id": "oHKfwlmlPD3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_percentage(num_uniques, total_rows):\n",
        "    percentage = (num_uniques / total_rows) * 100\n",
        "    rounded = round(percentage, 2)\n",
        "    return \"<0.01%\" if rounded == 0.0 and num_uniques > 0 else f\"{rounded:.2f}%\""
      ],
      "metadata": {
        "trusted": true,
        "id": "_sCK96wcPD3H"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def get_uniques_list(cols):\n",
        "    return sorted(\n",
        "        [(col, df[col].nunique()) for col in cols],\n",
        "        key=lambda x: x[1],\n",
        "        reverse=True)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "3CpIoNIOPD3I"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Comprobar cuántos valores son únicos en los registros (ID)\n",
        "\n",
        "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "string_cols = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "numeric_uniques = get_uniques_list(numeric_cols)\n",
        "string_uniques = get_uniques_list(string_cols)\n",
        "\n",
        "print(\"\\nValores únicos para cada columna numérica:\\n\")\n",
        "print(f\"{'Columna':<30} {'Únicos':<10} {'% sobre total'}\")\n",
        "print(\"-\" * 55)\n",
        "for col, num_uniques in numeric_uniques:\n",
        "    percentage_str = format_percentage(num_uniques, total_rows)\n",
        "    print(f\"{col:<30} {num_uniques:<10} {percentage_str}\")\n",
        "\n",
        "print(\"\\n\\nValores únicos para cada columna categórica:\\n\")\n",
        "print(f\"{'Columna':<30} {'Únicos':<10} {'% sobre total'}\")\n",
        "print(\"-\" * 55)\n",
        "for col, num_uniques in string_uniques:\n",
        "    percentage_str = format_percentage(num_uniques, total_rows)\n",
        "    print(f\"{col:<30} {num_uniques:<10} {percentage_str}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "Xtn6adRKPD3I"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Listado de columnas irrelevantes para la matriz de correlación\n",
        "# Selección en función de output anterior (>3%)\n",
        "irrelevant_columns = ['AlertId', 'Id', 'AccountUpn', 'IncidentId', 'NetworkMessageId',\n",
        "                     'AccountName', 'AccountSid', 'AccountObjectId', 'IpAddress']\n",
        "\n",
        "df = df.drop(columns=irrelevant_columns)\n",
        "\n",
        "viewRemainingColumns()"
      ],
      "metadata": {
        "trusted": true,
        "id": "_4QZweutPD3J"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. Codificación de variables categóricas"
      ],
      "metadata": {
        "id": "TzSCr6OVPD3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Seleccionar las columnas de tipo 'object', excluyendo 'Timestamp'\n",
        "encode_label = df.select_dtypes([object]).columns\n",
        "encode_label = [col for col in encode_label if col != 'Timestamp']\n",
        "\n",
        "print(encode_label)\n",
        "\n",
        "# Diccionario para almacenar los codificadores\n",
        "label_encoders = {}\n",
        "\n",
        "# Aplicar LabelEncoder a cada columna categórica, excepto 'Timestamp'\n",
        "for col in encode_label:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "wo0OjshNPD3K"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3. Tratamiento de fechas"
      ],
      "metadata": {
        "id": "jWdD4WAXPD3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3.1. Conversión de fechas\n"
      ],
      "metadata": {
        "id": "-p_7aFDTPD3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Procesar DateTime\n",
        "\n",
        "unique_lengths = set(df['Timestamp'].map(len))\n",
        "print(unique_lengths)\n",
        "\n",
        "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
        "\n",
        "df['Year'] = df['Timestamp'].dt.year\n",
        "df['Month'] = df['Timestamp'].dt.month\n",
        "df['Day'] = df['Timestamp'].dt.day\n",
        "df['Hour'] = df['Timestamp'].dt.hour\n",
        "df['Minute'] = df['Timestamp'].dt.minute\n",
        "df['Second'] = df['Timestamp'].dt.second\n",
        "df['Weekday'] = df['Timestamp'].dt.weekday  # 0 = Monday, 6 = Sunday"
      ],
      "metadata": {
        "trusted": true,
        "id": "v9PbmN-jPD3M"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3.2. Agrupamiento de fechas"
      ],
      "metadata": {
        "id": "RA4HOh-WPD3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Día: 0 | Noche: 1\n",
        "\n",
        "def asign_period(hour):\n",
        "    if 7 <= hour < 19:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "\n",
        "## Primavera: 0 | Verano: 1 | Otoño: 2 | Invierno: 3\n",
        "\n",
        "def asign_season(month):\n",
        "    if 3 <= month <= 5:\n",
        "        return 0\n",
        "    elif 6 <= month <= 8:\n",
        "        return 1\n",
        "    elif 9 <= month <= 11:\n",
        "        return 2\n",
        "    else:\n",
        "        return 3\n",
        "\n",
        "\n",
        "## Lectivo: 0 | No lectivo: 1\n",
        "\n",
        "def asign_holidays(weekday):\n",
        "    if 0 <= weekday < 5:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1"
      ],
      "metadata": {
        "trusted": true,
        "id": "iafJHtbuPD3N"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Por franja horaria\n",
        "df['Period'] = df['Hour'].apply(asign_period)\n",
        "\n",
        "# Por estaciones\n",
        "df['Season'] = df['Month'].apply(asign_season)\n",
        "\n",
        "# Por lectividad\n",
        "df['Holiday'] = df['Weekday'].apply(asign_holidays)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Oh58fLyfPD3O"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "viewRemainingColumns()"
      ],
      "metadata": {
        "trusted": true,
        "id": "OiefB94hPD3O"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3.3. Eliminación de atributos de fechas"
      ],
      "metadata": {
        "id": "B0U8d78rPD3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Valores únicos para el año:\", len(df['Year'].unique())) # Se puede eliminar esta variable"
      ],
      "metadata": {
        "trusted": true,
        "id": "gSTEfgxcPD3P"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns=['Year', 'Month', 'Weekday', 'Day', 'Hour', 'Minute', 'Second'])\n",
        "df = df.drop(columns=['Timestamp'])"
      ],
      "metadata": {
        "trusted": true,
        "id": "VVJTaKB1PD3Q"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "viewRemainingColumns()"
      ],
      "metadata": {
        "trusted": true,
        "id": "7IaVSeajPD3R"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4. Análisis de correlación"
      ],
      "metadata": {
        "id": "RObt39myPD3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4.1. Matriz de correlación"
      ],
      "metadata": {
        "id": "esoBGf0gPD3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular la matriz de correlación\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "# Visualización de la matriz de correlación\n",
        "plt.figure(figsize=(18, 16))\n",
        "ax = sns.heatmap(correlation_matrix, annot=False, cmap=\"coolwarm\", linewidths=0.5)\n",
        "plt.title(\"Matriz de correlación\")\n",
        "\n",
        "style_and_save(\n",
        "    ax=ax,\n",
        "    title='Matriz de Correlación',\n",
        "    xlabel='Variables',\n",
        "    ylabel='Variables',\n",
        "    filename='matriz_de_correlacion.png',\n",
        "    xtick_rotation=45,\n",
        "    ytick_rotation=45\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "4jtTRfwpPD3T"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4.2. Eliminación de variables muy correlacionadas"
      ],
      "metadata": {
        "id": "-HTiGgHBPD3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0.7\n",
        "\n",
        "abs_corr = correlation_matrix.abs()\n",
        "high_corr_pairs = []\n",
        "\n",
        "num_cols = abs_corr.shape[1]\n",
        "\n",
        "for i in range(num_cols):\n",
        "    for j in range(i + 1, num_cols):\n",
        "        corr_value = abs_corr.iloc[i, j]\n",
        "        if corr_value > threshold:\n",
        "            high_corr_pairs.append((abs_corr.columns[i], abs_corr.columns[j], corr_value))\n",
        "\n",
        "if high_corr_pairs:\n",
        "    print(f\"Parejas de variables con correlación mayor que {threshold}:\")\n",
        "    print(f\"{'Variable 1':<20} {'Variable 2':<20} {'Correlación'}\")\n",
        "    print(\"-\" * 55)\n",
        "    for col1, col2, corr_value in high_corr_pairs:\n",
        "        print(f\"{col1:<20} {col2:<20} {corr_value:.2f}\")\n",
        "else:\n",
        "    print(f\"No hay parejas con correlación mayor que {threshold}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "gWk0eWxOPD3T"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns=['Sha256', 'FileName', 'RegistryValueData', 'ApplicationName', 'OSVersion', 'City', 'State'])"
      ],
      "metadata": {
        "trusted": true,
        "id": "Ar-jdJo2PD3U"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "viewRemainingColumns()"
      ],
      "metadata": {
        "trusted": true,
        "id": "mil-kCGHPD3U"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5. Filtrado manual"
      ],
      "metadata": {
        "id": "rjeV0ZThPD3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns=['OrgId', 'DetectorId', 'DeviceId', 'ApplicationId', 'OAuthApplicationId'])"
      ],
      "metadata": {
        "trusted": true,
        "id": "6ksUGizDPD3V"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "viewRemainingColumns()"
      ],
      "metadata": {
        "trusted": true,
        "id": "znnrwFRQPD3V"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.6. Detección de anomalías\n",
        "### 3.5.1. Eliminar valores extremos (outliers)\n",
        "### 3.5.2. PCA"
      ],
      "metadata": {
        "id": "kG05G9jaPD3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Entrenamiento"
      ],
      "metadata": {
        "id": "8tw1mJGaPD3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1. Preparación del entrenamiento"
      ],
      "metadata": {
        "id": "da57nTPAPD3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1.1. Importaciones"
      ],
      "metadata": {
        "id": "DjB62FhAPD3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "trusted": true,
        "id": "GG5HHIjKPD3X"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1.2. Codificación de la variable objetivo"
      ],
      "metadata": {
        "id": "371lqmJsPD3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {\n",
        "    'BenignPositive': 0,\n",
        "    'FalsePositive': 1,\n",
        "    'TruePositive': 2\n",
        "}"
      ],
      "metadata": {
        "trusted": true,
        "id": "4Kf0I7iEPD3Y"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1.3. División de datos"
      ],
      "metadata": {
        "id": "LG69NOKbPD3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# X e y\n",
        "X = df.drop(columns=['IncidentGrade'])\n",
        "y = df['IncidentGrade']"
      ],
      "metadata": {
        "trusted": true,
        "id": "TSycpXFjPD3Y"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train, X_test, y_train, y_test = train_test_split(\n",
        "#    X, y, test_size=0.2, random_state=42, stratify=y\n",
        "#)"
      ],
      "metadata": {
        "trusted": true,
        "id": "xB75eRQ9PD3Z"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2. Modelos"
      ],
      "metadata": {
        "id": "exgjcnl7PD3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    'CatBoost': CatBoostClassifier(verbose=0),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),\n",
        "    'RandomForest': RandomForestClassifier()\n",
        "}"
      ],
      "metadata": {
        "trusted": true,
        "id": "uAFpEIy6PD3Z"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3. Entrenamiento"
      ],
      "metadata": {
        "id": "ryrZtahuPD3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for name, model in models.items():\n",
        "#    print(f\"\\nEntrenando {name}...\")\n",
        "#    model.fit(X_train, y_train)\n",
        "#    y_pred = model.predict(X_test)\n",
        "#    print(f\"\\n{name} Classification Report:\")\n",
        "#    print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "trusted": true,
        "id": "Rv41y36GPD3a"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4. Balanceo de clases"
      ],
      "metadata": {
        "id": "HkSw_b18PD3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import CondensedNearestNeighbour\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances_argmin_min\n",
        "from imblearn.under_sampling import ClusterCentroids\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.utils import resample\n",
        "import time"
      ],
      "metadata": {
        "trusted": true,
        "id": "vIrlaoDjPD3c"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "target_size = y.value_counts()[1]"
      ],
      "metadata": {
        "trusted": true,
        "id": "jtJblQ6kPD3d"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Undersample clase 0 (BenignPositive)\n",
        "rus_0 = RandomUnderSampler(sampling_strategy={0: target_size}, random_state=42)\n",
        "X_rus_0, y_rus_0 = rus_0.fit_resample(X, y)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Wn2kFEt2PD3e"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Oversample clase 1 (FalsePositive) sobre el resultado del paso anterior\n",
        "# smote = SMOTE(sampling_strategy={1: target_size}, random_state=42)\n",
        "# X_balanced, y_balanced = smote.fit_resample(X_rus, y_rus)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Ut8pfVMHPD3e"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Undersample clase 2 (BenignPositive)\n",
        "rus_2 = RandomUnderSampler(sampling_strategy={2: target_size}, random_state=42)\n",
        "X_rus_final, y_rus_final = rus_2.fit_resample(X_rus_0, y_rus_0)"
      ],
      "metadata": {
        "trusted": true,
        "id": "-lGgIsl0PD3f"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Mezclar datos\n",
        "X_balanced, y_balanced = shuffle(X_rus_final, y_rus_final, random_state=42)"
      ],
      "metadata": {
        "trusted": true,
        "id": "4UpO_HBcPD3g"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Verificar el resultado\n",
        "print(\"Distribución tras undersampling:\")\n",
        "print(y_balanced.value_counts())"
      ],
      "metadata": {
        "trusted": true,
        "id": "d_oa5C4uPD3g"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_balanced, y_balanced, test_size=0.2, stratify=y_balanced, random_state=42\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "TiG_4KcwPD3h"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#from imblearn.under_sampling import TomekLinks\n",
        "\n",
        "#tl = TomekLinks(sampling_strategy='auto')  # auto aplica a todas las clases\n",
        "#X_tomek, y_tomek = tl.fit_resample(X, y)\n",
        "\n",
        "# Verificar tamaños\n",
        "#from collections import Counter\n",
        "#print(\"Distribución original:\", Counter(y))\n",
        "#print(\"Distribución tras Tomek:\", Counter(y_tomek))"
      ],
      "metadata": {
        "trusted": true,
        "id": "8OFMiC0nPD3i"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# def extract_representatives_kmeans(X, y, n_samples_per_class=100):\n",
        "#     X_rep = []\n",
        "#     y_rep = []\n",
        "\n",
        "#     for label in np.unique(y):\n",
        "#         X_class = X[y == label]\n",
        "#         y_class = y[y == label]\n",
        "\n",
        "#         n_clusters = min(n_samples_per_class, len(X_class))\n",
        "\n",
        "#         kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "#         kmeans.fit(X_class)\n",
        "#         closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, X_class)\n",
        "\n",
        "#         X_selected = X_class.iloc[closest]\n",
        "#         y_selected = y_class.iloc[closest]\n",
        "\n",
        "#         X_rep.append(X_selected)\n",
        "#         y_rep.append(y_selected)\n",
        "\n",
        "#     X_balanced_kmeans = pd.concat(X_rep)\n",
        "#     y_balanced_kmeans = pd.concat(y_rep)\n",
        "\n",
        "#     return X_balanced_kmeans, y_balanced_kmeans"
      ],
      "metadata": {
        "trusted": true,
        "id": "0VvY6uhFPD3i"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicar balanceo por representatividad\n",
        "#X_kmeans, y_kmeans = extract_representatives_kmeans(X, y, n_samples_per_class=100)\n",
        "\n",
        "# Dividir para entrenamiento\n",
        "#X_train_km, X_test_km, y_train_km, y_test_km = train_test_split(\n",
        "#    X_kmeans, y_kmeans, test_size=0.2, stratify=y_kmeans, random_state=42\n",
        "#)\n",
        "\n",
        "# Entrenar y evaluar modelos\n",
        "#for name, model in models.items():\n",
        " #   print(f\"\\n{name} con KMeans representativo:\")\n",
        " #  model.fit(X_train_km, y_train_km)\n",
        " # y_pred_km = model.predict(X_test_km)\n",
        " #print(classification_report(y_test_km, y_pred_km))"
      ],
      "metadata": {
        "trusted": true,
        "id": "yB30IKHJPD3i"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# df_full = pd.concat([X, y], axis=1)\n",
        "\n",
        "# df_sampled = df_full.groupby('IncidentGrade').apply(\n",
        "#     lambda g: g.sample(n=min(len(g), 100000), random_state=42)\n",
        "# ).reset_index(drop=True)\n",
        "\n",
        "# X_sampled = df_sampled.drop(columns='IncidentGrade')\n",
        "# y_sampled = df_sampled['IncidentGrade']\n",
        "\n",
        "# mini_kmeans = MiniBatchKMeans(n_init=10, batch_size=1024, max_iter=100, random_state=42)\n",
        "\n",
        "# cc = ClusterCentroids(estimator=mini_kmeans, random_state=42)\n",
        "\n",
        "# X_cc, y_cc = cc.fit_resample(X_sampled, y_sampled)\n",
        "\n",
        "# print(\"\\nDistribución de clases tras ClusterCentroids:\")\n",
        "# print(pd.Series(y_cc).value_counts())\n",
        "\n",
        "# # División en train y test\n",
        "# X_train_cc, X_test_cc, y_train_cc, y_test_cc = train_test_split(\n",
        "#     X_cc, y_cc, test_size=0.2, stratify=y_cc, random_state=42\n",
        "# )"
      ],
      "metadata": {
        "trusted": true,
        "id": "TzLKIB3SPD3i"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"\\nModelos tras balanceo:\")\n",
        "\n",
        "# for name, model in models.items():\n",
        "#     print(f\"\\n✅ {name} con ClusterCentroids:\")\n",
        "#     model.fit(X_train_cc, y_train_cc)\n",
        "#     y_pred_cnn = model.predict(X_test_cc)\n",
        "#     print(f\"\\n{name} Classification Report:\")\n",
        "#     print(classification_report(y_test_cc, y_pred_cc))"
      ],
      "metadata": {
        "trusted": true,
        "id": "JybUhUpxPD3k"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# for name, model in models.items():\n",
        "#    print(f\"\\nEntrenando {name}...\")\n",
        "#    model.fit(X_train, y_train)\n",
        "#    y_pred = model.predict(X_test)\n",
        "#    print(f\"\\n{name} Classification Report:\")\n",
        "#    print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "trusted": true,
        "id": "u5OMtdxMPD3k"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5. Ensemble y stacking"
      ],
      "metadata": {
        "id": "VfnhI-oXPD3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "trusted": true,
        "id": "vpXrKM2lPD3l"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('catboost', models['CatBoost']),\n",
        "        ('xgboost', models['XGBoost']),\n",
        "        ('randomforest', models['RandomForest'])\n",
        "    ],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "ensemble.fit(X_train, y_train)\n",
        "\n",
        "y_pred_ensemble = ensemble.predict(X_test)\n",
        "print(\"\\nEnsemble con Soft Voting sobre datos balanceados:\")\n",
        "print(classification_report(y_test, y_pred_ensemble))"
      ],
      "metadata": {
        "trusted": true,
        "id": "ifKDwd8oPD3l"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# stacking_clf = StackingClassifier(\n",
        "#     estimators=[\n",
        "#         ('catboost', models['CatBoost']),\n",
        "#         ('xgboost', models['XGBoost']),\n",
        "#         ('randomforest', models['RandomForest'])\n",
        "#     ],\n",
        "#     final_estimator=LogisticRegression(max_iter=1000),\n",
        "#     cv=5,\n",
        "#     n_jobs=-1,\n",
        "#     passthrough=False\n",
        "# )\n",
        "\n",
        "# stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# y_pred_stack = stacking_clf.predict(X_test)\n",
        "# print(\"\\nStacking con Logistic Regression como meta-modelo:\")\n",
        "# print(classification_report(y_test, y_pred_stack))"
      ],
      "metadata": {
        "trusted": true,
        "id": "6ggjV68DPD3m"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Evaluación"
      ],
      "metadata": {
        "id": "penS1KRjPD3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import numpy as np"
      ],
      "metadata": {
        "trusted": true,
        "id": "5hMDdFOXPD3o"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Binarizar etiquetas para clasificación multiclase\n",
        "classes = np.unique(y_test)\n",
        "y_test_bin = label_binarize(y_test, classes=classes)\n",
        "n_classes = y_test_bin.shape[1]\n",
        "\n",
        "# Obtener probabilidades\n",
        "y_proba_ensemble = ensemble.predict_proba(X_test)\n",
        "# y_proba_stack = stacking_clf.predict_proba(X_test)"
      ],
      "metadata": {
        "trusted": true,
        "id": "zYYusXkVPD3p"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1. Curva ROC - AUC"
      ],
      "metadata": {
        "id": "2G28IDBfPD3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular curvas ROC y AUC por clase\n",
        "fpr_ens, tpr_ens, fpr_stack, tpr_stack, auc_ens, auc_stack = {}, {}, {}, {}, {}, {}\n",
        "for i in range(n_classes):\n",
        "    fpr_ens[i], tpr_ens[i], _ = roc_curve(y_test_bin[:, i], y_proba_ensemble[:, i])\n",
        "    # fpr_stack[i], tpr_stack[i], _ = roc_curve(y_test_bin[:, i], y_proba_stack[:, i])\n",
        "    auc_ens[i] = auc(fpr_ens[i], tpr_ens[i])\n",
        "    # auc_stack[i] = auc(fpr_stack[i], tpr_stack[i])"
      ],
      "metadata": {
        "trusted": true,
        "id": "t-DL1hayPD3s"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot ROC\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i in range(n_classes):\n",
        "    plt.plot(fpr_ens[i], tpr_ens[i], '--', label=f'Ensemble - Clase {classes[i]} (AUC = {auc_ens[i]:.2f})')\n",
        "    # plt.plot(fpr_stack[i], tpr_stack[i], '-', label=f'Stacking - Clase {classes[i]} (AUC = {auc_stack[i]:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "plt.title('Curva ROC - Ensemble')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "DJv23oB2PD3s"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2. Matriz de confusión"
      ],
      "metadata": {
        "id": "8O8SIendPD3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Matriz de confusión - Ensemble\n",
        "cm_ens = confusion_matrix(y_test, y_pred_ensemble)\n",
        "disp_ens = ConfusionMatrixDisplay(confusion_matrix=cm_ens, display_labels=classes)\n",
        "disp_ens.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Matriz de Confusión - Ensemble\")\n",
        "plt.show()\n",
        "\n",
        "# Matriz de confusión - Stacking\n",
        "# cm_stack = confusion_matrix(y_test, y_pred_stack)\n",
        "# disp_stack = ConfusionMatrixDisplay(confusion_matrix=cm_stack, display_labels=classes)\n",
        "# disp_stack.plot(cmap=plt.cm.Oranges)\n",
        "# plt.title(\"Matriz de Confusión - Stacking\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "wMTXd11-PD3t"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2. Precisión, Recall, F1-Score"
      ],
      "metadata": {
        "id": "N5A07ijmPD3u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4. Distribución de las probabilidades predichas por clase"
      ],
      "metadata": {
        "id": "VxF8O3W-PD3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd"
      ],
      "metadata": {
        "trusted": true,
        "id": "txZiXNCMPD3u"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear DataFrame con probabilidades predichas por el ensemble y stacking\n",
        "ensemble_proba_df = pd.DataFrame(y_proba_ensemble, columns=[f'Clase {c}' for c in classes])\n",
        "# stacking_proba_df = pd.DataFrame(y_proba_stack, columns=[f'Clase {c}' for c in classes])\n",
        "\n",
        "# Añadir etiquetas verdaderas para distinguir las clases\n",
        "ensemble_proba_df['Clase verdadera'] = y_test.values\n",
        "# stacking_proba_df['Clase verdadera'] = y_test.values"
      ],
      "metadata": {
        "trusted": true,
        "id": "XOOG2DaDPD3u"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Gráficas por clase\n",
        "for c in classes:\n",
        "    clase_str = f'Clase {c}'\n",
        "    plt.figure(figsize=(10, 4))\n",
        "\n",
        "    # Ensemble\n",
        "    sns.kdeplot(\n",
        "        data=ensemble_proba_df, x=clase_str, hue='Clase verdadera',\n",
        "        common_norm=False, fill=True, alpha=0.4, linewidth=1.5,\n",
        "        palette='tab10', label='Ensemble'\n",
        "    )\n",
        "    plt.title(f'Distribución de probabilidades - Ensemble ({clase_str})')\n",
        "    plt.xlabel('Probabilidad estimada')\n",
        "    plt.ylabel('Densidad')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # # Stacking\n",
        "    # plt.figure(figsize=(10, 4))\n",
        "    # sns.kdeplot(\n",
        "    #     data=stacking_proba_df, x=clase_str, hue='Clase verdadera',\n",
        "    #     common_norm=False, fill=True, alpha=0.4, linewidth=1.5,\n",
        "    #     palette='tab10', label='Stacking'\n",
        "    # )\n",
        "    # plt.title(f'Distribución de probabilidades - Stacking ({clase_str})')\n",
        "    # plt.xlabel('Probabilidad estimada')\n",
        "    # plt.ylabel('Densidad')\n",
        "    # plt.grid(True)\n",
        "    # plt.tight_layout()\n",
        "    # plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "m4u9Dc0HPD3v"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}